# Graph Embedding and Evaluating Framework

The general purpose of this framework is to embed graphs and evaluate these embeddings using various evaluation metrics.

## Running Experiments

### Using a Docker Container

To run the experiments, we recommend using Docker. Docker simplifies the process of packaging and managing the framework across different environments, ensuring consistency and reliability.

For details on utilizing the implemented Dockerfile, refer to [docker/README.md](docker/README.md).

If you decide to run the experiments natively without Docker, make sure to install all the required packages. The [Dockerfile](docker/dockerfile) used to create the Docker image can also be used as a list of packages required for the framework to run.

### Setting up Experiments

This framework comprises three main parts:

1. `embedding/`: This directory contains the implemented graph embedders. For information on how to use these embedders, configure them, and add new ones, consult [embedding/README.md](embedding/README.md).

2. `evaluation/`: This directory contains various evaluation metrics that can be used to evaluate a graph embedding based on similarity metrics. For more details about evaluations, see [evaluation/README.md](evaluation/README.md).

3. `data/`: This directory is primarily used for storing data used and generated by experiments. It includes:
    - `data/input_data/`: This directory contains the edgelists of graphs to be embedded and possibly evaluated. Note that for each edgelist, a value `"group"` is defined, and each edgelist can be in a subdirectory in `data/input_data/` or directly in `data/input_data/`. In the first case, the group of the edgelist would be the name of the subdirectory; otherwise, it's simply the name of the edgelist. It's important to note that each edgelist data should represent a connected graph and consist of lines of node pairs that represent edges. There are no restrictions on the node names; they can also be strings
    - `data/config/`: This directory contains configurations for embeddings (see [embedding/README.md](embedding/README.md)) and experiment configurations.
    - `data/embedding_result/`: This directory is used to save the results of embedding for each given embedder and edgelist.
    - `data/evaluation_result/`: This directory is used to save the values calculated by evaluation metrics.
    - `data/output/`: This directory is used to save the resulting output, such as unified `.csv` data.

To configure the experiments, create a `data/config/main.json` file in the following JSON format:

```json
{
    "embeddings": [
        "Spring",
        "Node2Vec",
        "Struc2Vec",
        "Verse"
    ],
    "evaluations": {
        "average_error_link_prediction": [
            "EuclidianDistance",
            "InnerProduct"
        ],
        "precision_at_k_link_prediction": [
            "EuclidianDistance",
            "InnerProduct"
        ],
        "read_time": [
            "None"
        ]
    },
    "similarity_metrics": {
        "spring": [
            "EuclidianDistance",
            "None"
        ],
        "node2vec": [
            "EuclidianDistance",
            "None"
        ],
        "struc2vec": [
            "EuclidianDistance",
            "None"
        ],
        "verse": [
            "EuclidianDistance",
            "InnerProduct",
            "None"
        ]
    },
    "csv_per_dir": true,
    "num_parallel_runs": 4
}
```

* `"embeddings"` lists the embeddings to be used for graph embedding. Note that the names of embedders should be identical to their class names (e.g. `"Node2Vec"` instead of `"node2vec"`, see [node2vec.py](embedding/node2vec/node2vec.py)).
* `"evaluations"` maps evaluation metrics to their compatible similarity metrics, e.g. `"read_time"` only reads time and is independent of similarity metrics, thus `"None"`.
* `"similarity_metric"` maps each embedding variation, which represents an embedding method with different configuration settings (e.g. 'spring -d 10' is an embedding variation of the embedding 'spring'), to similarity metrics used for evaluation. Note that for each embedding variation:
    1. If a value exists, take it from the configuration settings for this embedding variation, else:
    2. check for a value in the configuration settings of the simplified version of this variation (e.g., 'spring -d 10' -> 'spring').
    3. If a value is not found in either of the above cases, the system defaults to the default similarity_metric.
    4. If none of the above conditions are met, an empty list is used. This means that this embedding variation won't be evaluated.
* `"csv_per_dir"` determines whether there should be a `.csv` file for every group.
* `"num_parallel_runs"` detemines the number of experiments that can run in parallel. It sets the value for `run.use_cores()`, for further information, see below.

### Running the Experiments

We use the python module [run](https://github.com/thobl/run) for running the experiments.

To calculate embeddings for input graphs, use:

```terminal
python ./main.py layout
```

To evaluate the embeddings, use:

```terminal
python ./main.py evaluate
```

Or execute both operations simultaneously with:

```
python ./main.py layout evaluate
```

Additionally, if you are interested in generating the examples that we provide, use the following command:

```terminal
example/setup.sh
```

Our examples include four different graph groups, each with 50 samples, and Zacharyâ€™s Karate Club graph as a standalone graph. For additional information, refer to [example/build_graph.py](example/build_graph.py).

In this context, we have also configured the use of two different variations of `node2vec`. For more details, see [example/build_config.py](example/build_config.py).

Note, that the example data directory will be stored in `example/data/`. If you want to run the experiments natively (without Docker), you'll need to move this directory to the root directory.

### Ploting

In addition to saving the evaluation result as a `.csv` file, we also provide an R script for plotting the evaluation result based on evaluation metrics. To use the R script, you need to install the required R packages. To do so, run the following command:

```terminal
R -e 'install.packages(c("dplyr", "tidyverse"), repos="https://cloud.r-project.org/")'
```

To execute the R script, use the following command in the root directory:

```terminal
evaluation/group_based.R data/ <evaluation1_name> <evaluation2_name> ...
```

Here, `<evaluation_name>` specifies the type of the evaluation value, e.g. `f_score` for `average_error_link_prediction` or `time` for `read_time`. Additionally, `data/` specifies the path to the data directory where the experiment results are stored.