# Graph Embedding and Evaluating Framework

The general purpose of this framework is to embed graphs and evaluate these embeddings using various evaluation metrics.

## Running Experiments

### Using a Docker Container

To run the experiments, we recommend using Docker. Docker simplifies the process of packaging and managing the framework across different environments, ensuring consistency and reliability.

For details on utilizing the implemented Dockerfile, refer to [docker/README.md](docker/README.md).

If you decide to run the experiments natively without Docker, make sure to install all the required packages. The [Dockerfile](docker/dockerfile) used to create the Docker image can also be used as a list of packages required for the framework to run.

### Setting up Experiments

This framework comprises three main parts:

1. `embedding/`: This directory contains the implemented graph embedders. For information on how to use these embedders, configure them, and add new ones, consult [embedding/README.md](embedding/README.md).

2. `evaluation/`: This directory contains various evaluation metrics that can be used to evaluate a graph embedding based on similarity metrics. For more details about evaluations, see [evaluation/README.md](evaluation/README.md).

3. `data/`: This directory is primarily used for storing data used and generated by experiments. It includes:
    - `data/input_data/`: This directory contains the edgelists of graphs to be embedded and possibly evaluated. Note that for each edgelist, a value `"group"` is defined, and each edgelist can be in a subdirectory in `data/input_data/` or directly in `data/input_data/`. In the first case, the group of the edgelist would be the name of the subdirectory; otherwise, it's simply the name of the edgelist.
    - `data/config/`: This directory contains configurations for embeddings (see [embedding/README.md](embedding/README.md)) and experiment configurations.
    - `data/embedding_result/`: This directory is used to save the results of embedding for each given embedder and edgelist.
    - `data/evaluation_result/`: This directory is used to save the values calculated by evaluation metrics.
    - `data/output/`: This directory is used to save the resulting output, such as unified `.csv` data.

To configure the experiments, create a `data/config/main.json` file in the following JSON format:

```json
{
    "embeddings": [
        "Spring",
        "Node2Vec",
        "Struc2Vec",
        "Verse"
    ],
    "evaluations": {
        "average_error_link_prediction": [
            "EuclidianDistance",
            "InnerProduct"
        ],
        "precision_at_k_link_prediction": [
            "EuclidianDistance",
            "InnerProduct"
        ],
        "read_time": [
            "None"
        ]
    },
    "similarity_metrics": {
        "spring": [
            "EuclidianDistance",
            "None"
        ],
        "node2vec": [
            "EuclidianDistance",
            "None"
        ],
        "struc2vec": [
            "EuclidianDistance",
            "None"
        ],
        "verse": [
            "EuclidianDistance",
            "InnerProduct",
            "None"
        ]
    },
    "csv_per_dir": true,
    "cores": 4
}
```

* `"embeddings"` lists the embeddings to be used for graph embedding. Note that the names of embedders should be identical to their class names (e.g. `"Node2Vec"` instead of `"node2vec"`, see [node2vec.py](embedding/node2vec/node2vec.py)).
* `"evaluations"` maps evaluation metrics to their compatible similarity metrics, e.g. `"read_time"` only reads time and is independent of similarity metrics, thus `"None"`.
* `"similarity_metric"` maps each embedding variation to similarity metrics used for evaluation. Note that for each embedding variation:
    1. If exists, take the value from config for this embedding variation, else:
    2. If exists, take the value from config for the simplified version of this variation (e.g. 'spring -d 10' -> 'spring'), else:
    3. If exists, take the value from default similarity_metric, else:
    4. An empty list
* `"csv_per_dir"` determines whether there should be a `.csv` file for every group.
* `"cores"` detemines the number of cores used by run.

### Running the Experiments

We use the python module [run](https://github.com/thobl/run) for running the experiments.

To calculate embeddings for input graphs, use:

```terminal
python ./main.py layout
```

To evaluate the embeddings, use:

```terminal
python ./main.py evaluate
```

Or execute both operations simultaneously with:

```
python ./main.py layout evaluate
```

### Ploting

In addition to saving the evaluation result as a `.csv` file, we also provide an R script for plotting the evaluation result based on evaluation metrics. To use the R script, you need to install the required R packages. To do so, run the following command:

```terminal
R -e 'install.packages(c("dplyr", "tidyverse"), repos="https://cloud.r-project.org/")'
```

To execute the R script, use the following command in the root directory:

```terminal
evaluation/group_based.R data/output/all_graphs.csv <evaluation1_name>#<evaluation1_type_of_value> <evaluation2_name>#<evaluation2_type_of_value> ...
```

Where `<evaluation_name>#<evaluation_type_of_value>` specifies the format of input, e.g. `average_error_link_prediction#f_score` or `read_time#time`

## Example
To load the examples, run the following command (after entering the Docker container, if you are using it):

```terminal
example/setup.sh
```

Our examples include four different graph groups, each with 50 samples, and Zacharyâ€™s Karate Club graph as a standalone graph. For additional information, refer to [example/build_graph.py](example/build_graph.py).

In this context, we have also configured the use of two different variations of `node2vec`. For more details, see [example/build_config.py](example/build_config.py).