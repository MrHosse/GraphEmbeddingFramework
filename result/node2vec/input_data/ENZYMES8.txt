An algorithmic framework for representational learning on graphs. [Jul  3 2023]
================================================================================
Input graph path (-i:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph
Output graph path (-o:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.emb
Number of dimensions. Default is 128 (-d:)=4
Length of walk per source. Default is 80 (-l:)=80
Number of walks per source. Default is 10 (-r:)=10
Context size for optimization. Default is 10 (-k:)=10
Number of epochs in SGD. Default is 1 (-e:)=1
Return hyperparameter. Default is 1 (-p:)=1
Inout hyperparameter. Default is 1 (-q:)=1
Verbose output. (-v)=YES
Graph is directed. (-dr)=YES
Graph is weighted. (-w)=YES
Output random walks instead of embeddings. (-ow)=NO
Read 133 lines from /home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph

Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 

Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%

Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 14.20% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 28.41% 
Learning Progress: 42.61% 
Learning Progress: 28.41% 
Learning Progress: 56.82% 
Learning Progress: 14.20% 
74,-0.0609801,-0.137104,-0.136882,0.069423
0,-0.890281,-1.36389,-4.04245,2.58393
22,0.12596,0.0714969,0.211976,-0.105895
3,1.32364,0.846197,0.753462,0.316251
12,0.868703,0.52412,1.7922,-0.0779735
7,1.30913,0.651475,1.87309,0.13663
4,1.25133,0.55288,1.58396,0.197869
70,0.460382,0.52352,0.552007,-0.162837
58,0.345991,0.585447,0.912542,-0.12462
11,0.997718,0.649907,1.7546,-0.0155476
8,1.43458,0.595623,1.90946,0.0764008
37,0.459305,0.425842,0.851254,-0.452176
36,0.558481,0.603856,0.766742,-0.261844
35,0.526207,0.600354,0.957002,-0.203421
53,0.867769,0.5534,1.69488,-0.146531
72,0.477769,0.562389,1.33377,-0.276962
71,0.683806,0.800419,1.71626,-0.196351
16,0.740298,0.721192,1.89127,-0.156831
15,0.614498,0.450655,1.6451,-0.117098
54,0.963416,0.521524,1.3349,-0.252324
57,0.251533,0.197779,0.656128,0.112431
55,0.680951,0.433221,0.786995,-0.241514
21,0.325001,0.429024,1.17039,-0.106519
20,0.0357442,-0.112037,-0.128831,0.217523
42,0.238274,0.475904,1.32374,-0.11054
26,0.61281,0.29636,1.60267,0.223591
24,0.542396,0.302369,1.38791,0.163959
23,0.466294,0.156111,1.0426,0.334683
9,0.322699,0.131965,0.657942,0.12142
41,0.0524772,0.133624,0.58029,0.0648413
5,0.242268,0.114272,0.495811,0.258885
47,-0.140829,0.010532,-0.120864,0.161868
46,0.177116,0.0595283,0.269299,0.120165
2,0.248314,0.021385,0.56487,0.381909
29,0.0855239,0.129813,0.0826072,-0.104363
52,0.130333,0.411852,0.421431,0.0597556
40,0.102064,0.423068,0.522778,0.154152
1,0.234445,0.202161,0.270164,0.126816
43,0.16975,0.339731,0.46542,0.266761
25,0.228164,0.0906847,0.89584,0.240182
88,-0.0263997,0.220163,0.566331,-0.247089
51,0.143785,0.417714,0.44739,-0.207614
33,0.11368,0.347343,0.368426,-0.179822
76,-0.227997,0.209075,0.689947,-0.122196
45,-0.143744,0.148641,0.280083,-0.260694
18,0.674086,0.414012,1.90078,0.048095
17,0.584888,0.245049,1.73917,0.156765
75,0.10475,-0.132892,0.329012,-0.280075
19,0.553891,0.573208,1.84647,-0.0668621
44,0.0742268,0.080882,0.644852,-0.0624417
60,0.0570218,0.287856,0.724907,-0.336976
67,0.150701,-0.241925,0.167096,0.142324
65,-0.107571,-0.147554,-0.238837,0.268176
30,0.169574,0.2388,0.500081,0.0304863
28,0.113913,0.181998,0.412406,0.0289243
6,-0.266333,-0.0796397,-0.158341,0.0187182
14,0.157002,0.0806378,0.138467,-0.0117931
31,0.0719571,0.0875183,0.211757,0.132365
87,-0.0924139,-0.0243337,0.427605,-0.0370759
83,-0.0158801,-0.0656874,-0.253601,0.0472377
56,0.0202348,0.0947349,-0.197944,0.109477
84,0.180589,0.126343,0.446427,4.52393e-06
82,0.165138,0.122276,0.379029,-0.0701625
63,-0.160703,0.0403126,0.24073,-0.199887
62,0.079242,0.166446,0.557183,-0.491149
13,-0.1094,-0.200539,-0.195676,0.207147
66,0.181659,-0.110481,0.367037,0.07957
64,0.0832104,-0.00800555,0.353523,0.168695
50,0.0254518,0.00929304,-0.0800582,0.13002
38,0.0554667,-0.0262602,-0.216346,0.191996
27,0.105792,0.154874,0.774691,0.0960186
32,0.0326268,0.0942418,0.0544529,0.0138941
49,0.140768,-0.0506963,0.0474044,0.00561681
48,0.1149,0.192906,0.184431,0.0374263
34,0.0687778,0.163939,0.337832,-0.0654513
73,0.254353,0.265178,0.845571,0.0244965
79,-0.109449,0.0466369,0.38143,0.072073
77,-0.127407,0.291299,0.31607,-0.0541674
10,-0.186586,-0.0658957,0.0298288,0.123013
61,-0.0227366,0.139624,0.347497,0.109843
78,0.261589,0.182259,1.02741,-0.0157744
80,0.179508,0.133749,0.715856,-0.131351
68,-0.0452326,0.0976974,0.0658389,0.133978
85,0.229034,0.136362,0.380853,-0.082648
81,-0.0572835,0.107431,0.508681,-0.125381
86,-0.0606141,-0.0823939,-0.0786709,-0.12858
69,0.101152,0.16716,0.351916,-0.172385
59,0.222706,0.0810322,0.368939,-0.072179
39,0.404618,0.34637,0.44282,-0.113783
