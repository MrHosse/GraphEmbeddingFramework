An algorithmic framework for representational learning on graphs. [Jul  3 2023]
================================================================================
Input graph path (-i:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph
Output graph path (-o:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.emb
Number of dimensions. Default is 128 (-d:)=4
Length of walk per source. Default is 80 (-l:)=80
Number of walks per source. Default is 10 (-r:)=10
Context size for optimization. Default is 10 (-k:)=10
Number of epochs in SGD. Default is 1 (-e:)=1
Return hyperparameter. Default is 1 (-p:)=1
Inout hyperparameter. Default is 1 (-q:)=1
Verbose output. (-v)=YES
Graph is directed. (-dr)=YES
Graph is weighted. (-w)=YES
Output random walks instead of embeddings. (-ow)=NO
Read 148 lines from /home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph

Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 

Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%

Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 16.03% 
Learning Progress: 16.03% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 32.05% 
Learning Progress: 0.00% 
Learning Progress: 48.08% 
Learning Progress: 48.08% 
Learning Progress: 64.10% 
Learning Progress: 64.10% 
75,0.00109779,-0.0263697,0.155871,-0.376621
0,0.0167116,0.367174,-0.41796,-4.08032
67,0.114754,-0.224117,-0.100664,-0.676517
77,0.0319722,-0.104665,0.083043,-0.544361
25,0.269704,-0.16592,-0.127126,-0.169959
45,0.389097,-0.262906,-0.30265,-0.618072
73,0.184998,-0.1869,0.00274991,-0.374534
7,0.261205,-0.273131,-0.290763,-0.171228
38,0.175853,-0.105467,-0.242689,-0.422047
58,0.256283,0.050562,0.281682,-0.421222
15,0.231934,-0.281308,-0.426337,-0.296606
26,0.256256,-0.26374,-0.447617,-0.411448
46,0.258945,-0.298513,-0.411003,-0.693252
27,0.117166,-0.216226,-0.298267,-0.380145
47,0.0532701,-0.252828,-0.417674,-0.54434
78,0.103317,-0.270985,-0.221043,-0.701073
68,0.181374,-0.186441,-0.177971,-0.574055
76,0.0757057,-0.192244,-0.0808937,-0.475457
35,0.224777,-0.290436,-0.412597,-0.73576
57,0.0509967,-0.169807,-0.00106981,-0.390502
39,0.161581,-0.227702,-0.171137,-0.386779
70,0.00171562,-0.189834,-0.0420495,-0.568759
61,0.0405848,0.0761433,0.118521,-0.385675
54,-0.122552,0.0823294,0.0477715,-0.382474
36,0.152037,-0.352822,-0.396405,-0.586233
74,0.102528,0.025369,0.127371,-0.44317
22,0.124938,-0.132985,-0.248442,-0.221584
13,0.184656,-0.156902,-0.37611,-0.211984
9,0.188309,-0.255404,-0.324557,-0.225568
40,0.129038,-0.152878,-0.255962,-0.397318
66,0.159098,-0.00724834,0.350926,-0.390125
48,0.118662,-0.35784,-0.441551,-0.643202
69,0.0370392,-0.160442,0.0601589,-0.667609
62,0.279226,-0.0516588,0.210976,-0.388901
60,0.157795,-0.099395,0.00496651,-0.551052
72,-0.0145164,-0.106318,0.135342,-0.359597
64,0.116188,-0.0762098,-0.00910159,-0.442061
19,0.248793,-0.379554,-0.371402,-0.256564
50,0.0237127,-0.455835,-0.32448,-0.27302
30,0.00351704,-0.344684,-0.304574,-0.416128
8,0.322529,-0.247882,-0.567238,-0.554371
4,0.234241,-0.37416,-0.543179,-0.438086
34,0.125139,-0.278703,-0.399251,-0.354324
43,0.448266,-0.289616,-0.479065,-0.202829
11,0.156961,-0.193202,-0.406587,-0.357946
44,0.235291,-0.187166,-0.2625,-0.186442
56,0.0571965,-0.16431,-0.158161,-0.716328
18,0.208563,-0.44621,-0.58885,-0.367602
1,0.26495,-0.217355,-0.448259,-0.056104
33,0.349805,-0.201636,-0.214889,-0.352764
10,0.155231,-0.281759,-0.371642,-0.22663
49,0.0314753,-0.329198,-0.369004,-0.528485
29,0.127443,-0.136449,-0.286516,-0.594711
51,0.117781,-0.11283,0.387717,-0.442695
59,0.0172491,-0.1411,0.0235061,-0.285315
16,0.0612142,-0.416918,-0.447488,-0.328535
42,0.0840101,-0.263782,-0.25145,-0.315802
23,0.037907,-0.292217,-0.398525,-0.0703775
24,0.453687,-0.213163,-0.485718,-0.498144
52,0.265223,-0.075095,0.132508,-0.53823
37,0.113498,-0.330596,-0.126108,-0.261465
2,0.862599,-4.12515,-1.8557,0.78162
3,-1.32552,-0.997608,-5.23943,-0.0816602
31,0.170581,-0.0195905,-0.257586,-0.436234
17,0.293797,-0.264472,-0.402502,-0.221751
20,0.103409,-0.311681,-0.443908,-0.475114
28,-0.038687,-0.0842313,-0.293347,-0.288973
12,0.396847,-0.171932,-0.606681,-0.474051
21,0.212101,-0.277201,-0.550492,-0.403364
53,0.131886,0.140751,0.325574,-0.424817
71,0.0957312,0.0525662,0.0854355,-0.260525
41,0.0857495,-0.335067,-0.131498,-0.464735
6,0.0793212,-0.304061,-0.369655,-0.169568
14,0.270234,-0.272854,-0.55594,-0.272865
32,0.0234299,-0.447448,-0.43292,-0.504408
55,0.116521,0.0661433,0.21767,-0.432251
5,0.299341,-0.209897,-0.388894,-0.315717
65,0.156302,0.128142,0.259076,-0.390455
63,0.0340507,0.00686039,-0.200064,-0.661742
