An algorithmic framework for representational learning on graphs. [Jul  3 2023]
================================================================================
Input graph path (-i:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph
Output graph path (-o:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.emb
Number of dimensions. Default is 128 (-d:)=4
Length of walk per source. Default is 80 (-l:)=80
Number of walks per source. Default is 10 (-r:)=10
Context size for optimization. Default is 10 (-k:)=10
Number of epochs in SGD. Default is 1 (-e:)=1
Return hyperparameter. Default is 1 (-p:)=1
Inout hyperparameter. Default is 1 (-q:)=1
Verbose output. (-v)=YES
Graph is directed. (-dr)=YES
Graph is weighted. (-w)=YES
Output random walks instead of embeddings. (-ow)=NO
Read 30 lines from /home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph

Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 

Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%

Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 39.06% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 78.12% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
8,-0.0419583,0.171288,-0.0847952,0.116092
0,1.41928,3.51999,-3.68529,1.43908
7,0.0697442,0.095847,-0.0591036,-0.00195328
14,0.0672589,0.0212413,-0.0960647,0.0598113
29,-0.0326411,0.00234708,-0.0937666,0.00373307
15,0.0939903,0.0925997,-0.0355113,0.211811
20,0.14685,0.168646,-0.117396,-0.0294362
23,-0.0362109,-0.0424916,0.1397,0.0572169
21,-0.13878,-0.249048,0.461453,-0.116662
4,0.0103007,-0.0603056,-0.300624,0.175661
9,-0.0638529,0.0344068,-0.148602,0.148035
28,0.0486217,0.116519,-0.0369064,0.0140305
32,0.0183195,-0.0821231,0.0647204,0.065011
19,0.17273,0.0321753,-0.106419,0.125416
25,-0.15311,-0.0219856,-0.119655,0.0459311
5,0.0205876,0.0800318,-0.116139,0.0169747
17,-0.0988068,0.0227076,-0.13911,0.125426
10,-0.0836226,-0.0215961,-0.18269,0.0648502
31,-0.134546,-0.0980407,0.167652,0.0554679
24,0.0224611,-0.0715164,-0.0924417,-0.0204173
11,0.00831703,0.0148467,0.0070273,0.0675837
3,-0.0557022,-0.0710444,-0.210144,0.0175481
18,-0.0528334,0.0158089,-0.103745,0.21033
30,0.0572049,0.0573031,-0.0913357,-0.0528533
12,-0.0321951,0.0723209,-0.0583506,0.09547
22,-0.0681858,0.0559674,0.0197265,0.137519
13,0.0262373,0.0459428,-0.176709,0.019606
27,-0.0728962,0.100445,-0.142904,0.101397
1,-0.0804033,0.0330724,-0.308404,0.0639191
16,0.0547959,-0.0464785,-0.284628,0.279373
6,-0.0352081,-0.0524291,-0.0480393,0.0190538
26,-0.122498,-0.0452886,-0.104817,0.0477569
2,-0.0743414,0.0449463,-0.186953,0.0403154
