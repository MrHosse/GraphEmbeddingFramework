An algorithmic framework for representational learning on graphs. [Jul  3 2023]
================================================================================
Input graph path (-i:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph
Output graph path (-o:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.emb
Number of dimensions. Default is 128 (-d:)=4
Length of walk per source. Default is 80 (-l:)=80
Number of walks per source. Default is 10 (-r:)=10
Context size for optimization. Default is 10 (-k:)=10
Number of epochs in SGD. Default is 1 (-e:)=1
Return hyperparameter. Default is 1 (-p:)=1
Inout hyperparameter. Default is 1 (-q:)=1
Verbose output. (-v)=YES
Graph is directed. (-dr)=YES
Graph is weighted. (-w)=YES
Output random walks instead of embeddings. (-ow)=NO
Read 28 lines from /home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph

Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 

Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%

Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 27.17% 
Learning Progress: 54.35% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
19,-0.114526,0.00947295,-0.0552872,0.0411793
15,0.372028,0.0336514,0.0507605,-0.104591
0,3.17184,1.76252,-0.461689,-5.17222
35,-0.175636,0.0607568,-0.107233,0.0186339
32,0.166247,0.0719094,-0.117037,-0.117844
26,-0.149973,-0.0506865,0.0939214,0.0117995
8,0.15797,0.151094,0.0482141,-0.149233
23,0.0334612,-0.0733105,-0.0194205,-0.0416789
20,0.163206,0.272447,0.0141699,-0.162031
30,-0.554189,-0.237425,-0.0378333,0.340662
22,-0.411989,-0.310319,0.0302014,0.0756471
3,0.0782855,-0.0645233,0.0527192,-0.081081
9,0.218528,0.216939,-0.0333011,-0.235778
5,0.323711,0.107137,-0.0475864,-0.194332
11,0.290453,0.0901145,-0.12867,-0.150198
4,-0.0689641,-0.0492176,-0.109647,-0.0171169
2,0.276588,0.184559,-0.0169967,-0.153173
27,0.132437,0.0280205,-0.0518548,-0.0894538
17,0.310151,0.258041,-0.0686594,-0.277428
34,-0.0917355,-0.0469471,-0.0499677,0.0733405
21,0.319261,0.141718,-0.128475,-0.146516
18,-0.0650836,-0.100082,0.0789386,0.0194017
10,-0.0627146,-0.131225,-0.0981142,-0.132069
43,0.328976,0.125268,-0.136607,-0.331144
6,0.128122,0.195453,-0.0767313,-0.120328
40,0.230114,0.0880023,-0.0788614,-0.22677
13,-0.0119923,-0.0331839,0.00659401,-0.0635321
12,0.158178,-0.0295347,-0.029104,-0.139553
1,0.300901,0.206142,-0.0910381,-0.261186
36,0.0324021,-0.0302781,-0.0873138,-0.0531256
14,0.321984,0.121007,0.018888,-0.284587
31,-0.0759411,0.0899913,-0.066365,-0.146174
28,0.295983,0.13108,0.0315723,-0.30751
37,-0.0418364,-0.030024,-0.0928127,-0.10961
44,-0.0605492,0.0836823,-0.05744,0.0295284
29,0.35603,0.184977,0.0643562,-0.125684
25,-0.11678,0.0165761,0.11301,-0.0338417
16,0.175187,0.141413,-0.0502658,-0.309605
39,-0.0386252,-0.0300278,0.117573,-0.0190315
7,0.0373349,-0.0519211,-0.106475,-0.101236
42,-0.398626,-0.260283,0.0709172,0.352117
38,-0.448527,-0.236646,-0.0504947,0.290452
41,-0.000640508,0.0635495,-0.0319227,0.0616453
46,0.0184056,0.0580719,0.0412002,-0.00380379
24,0.370337,0.343088,0.0345422,-0.192447
33,-0.0356572,-0.00722729,-0.000417605,0.0247218
45,0.0408909,0.0489066,0.0784082,-0.0713349
