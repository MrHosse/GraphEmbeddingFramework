An algorithmic framework for representational learning on graphs. [Jul  3 2023]
================================================================================
Input graph path (-i:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph
Output graph path (-o:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.emb
Number of dimensions. Default is 128 (-d:)=4
Length of walk per source. Default is 80 (-l:)=80
Number of walks per source. Default is 10 (-r:)=10
Context size for optimization. Default is 10 (-k:)=10
Number of epochs in SGD. Default is 1 (-e:)=1
Return hyperparameter. Default is 1 (-p:)=1
Inout hyperparameter. Default is 1 (-q:)=1
Verbose output. (-v)=YES
Graph is directed. (-dr)=YES
Graph is weighted. (-w)=YES
Output random walks instead of embeddings. (-ow)=NO
Read 127 lines from /home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph

Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 

Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%

Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 13.89% 
Learning Progress: 13.89% 
Learning Progress: 27.78% 
Learning Progress: 41.67% 
Learning Progress: 41.67% 
Learning Progress: 55.56% 
Learning Progress: 55.56% 
18,-0.247048,0.141037,-0.580368,-0.588805
5,0.00509142,-0.0461936,-0.642027,-0.496173
0,0.287242,-0.641683,4.97387,3.69665
84,-0.0514967,0.195527,-0.191317,-0.151939
83,-0.215727,0.163855,-0.0976987,-0.0902931
24,-0.104459,0.178877,0.08412,0.172053
60,0.0638472,-0.0241616,-0.256399,-0.074913
59,-0.0960048,-0.070435,-0.0259288,-0.0910402
15,0.122634,-0.0159533,0.0572849,0.0103225
78,-0.0501799,0.0324133,-0.271053,-0.0672405
76,-0.0479516,-0.166119,-0.239242,-0.258069
75,-0.068344,-0.0827806,-0.240356,-0.26656
21,-0.0556316,-0.0827154,-0.0842702,-0.134395
47,-0.116931,0.175645,-0.232997,-0.148029
10,-0.270869,0.725024,-0.846603,-0.6728
8,-0.336733,0.615172,-0.71933,-0.479496
7,-0.28953,0.409599,-0.425884,-0.183756
66,0.0882304,0.00118267,-0.0235815,0.0151778
17,-0.0442291,0.104416,-0.330255,-0.226433
16,0.013292,-0.0166987,-0.14016,0.0425866
57,0.0437344,-0.152707,0.0407702,-0.132601
56,-0.0543619,-0.0863344,-0.180513,-0.111104
12,0.044719,-0.0493627,0.0831963,0.210399
72,0.0983552,-0.0559026,-0.214565,-0.286714
40,-0.112907,0.281413,-0.818978,-0.482926
2,0.102617,-0.0344519,-0.0479299,0.118885
28,0.120822,-0.180796,-0.180665,-0.203855
27,0.0476875,-0.169062,-0.0571165,-0.173382
82,0.0283088,-0.0613861,-0.0204732,0.0521913
23,0.0524653,0.13601,-0.274479,-0.0464945
4,0.058201,0.0184574,0.0688245,-0.0236946
33,-0.118537,-0.0988864,-0.0124396,-0.0582243
32,0.0678916,-0.270469,-0.559198,-0.511405
36,-0.0479896,0.373391,-0.368967,-0.410103
35,-0.12931,0.269173,-0.387274,-0.417363
6,-0.0350608,0.219404,-0.608823,-0.558674
1,-0.104533,0.15544,-0.138723,-0.105703
19,0.0319037,0.166583,0.109768,0.127061
89,0.00246007,-0.0888987,-0.259327,-0.322504
88,-0.0590572,0.0301105,-0.439039,-0.264731
64,-0.0162363,-0.0743584,-0.526766,-0.282671
48,-0.103811,0.0598267,-0.681986,-0.600236
68,0.00295154,-0.128699,-0.529083,-0.504463
67,-0.186493,-0.111106,-0.682454,-0.486247
52,-0.108478,0.0510236,-0.165163,-0.231228
51,-0.101921,0.160834,-0.112378,-0.147703
62,-0.120632,0.0709933,-0.208127,-0.186202
55,0.109349,-0.0992814,0.0469613,-0.122639
13,0.100647,-0.147523,0.121035,0.135022
73,0.0781988,0.0840212,0.00788179,0.0278381
79,-0.133905,0.144121,-0.179002,-0.177614
9,-0.032463,0.0277919,0.0832912,0.0175505
3,-0.071232,-0.0778581,0.139226,0.071845
22,-0.0663109,-0.124168,0.111537,0.236068
77,0.0977655,-0.0168158,-0.0305986,-0.100483
42,-0.0828498,0.211265,-0.215748,-0.150986
34,0.0451138,0.103639,-0.207132,-0.130405
26,0.0711535,0.0659929,-0.345668,-0.255468
25,-0.110451,0.0459664,0.293189,0.121304
80,0.0173409,0.21073,-0.0649354,-0.272262
31,0.00342047,0.0702824,-0.103339,0.116629
70,-0.130462,0.0627405,-0.428773,-0.184735
14,0.0614585,-0.135893,-0.0940694,-0.126107
85,0.0177721,0.113767,0.0893687,-0.0995564
50,-0.0179067,0.188876,-0.396005,-0.338839
43,-0.0982475,0.00403896,-0.135809,0.0183416
30,0.0697504,0.120078,-0.200968,-0.0293437
29,0.072111,0.0562563,-0.0389916,-0.0501663
53,0.0728178,-0.00863512,0.10867,-0.00616003
39,-0.096385,-0.0148825,-0.0524735,0.0490474
63,-0.0269489,0.0891958,-0.212528,-0.0697496
69,-0.0884683,-0.0797173,-0.0474334,-0.125009
11,-0.104627,0.348081,-0.550831,-0.328968
45,-0.311136,0.532866,-0.320485,-0.277969
65,0.103099,-0.0456443,-0.373686,-0.223845
87,-0.124357,0.0606277,-0.0868338,-0.0621985
86,-0.178415,0.142631,-0.111564,-0.234035
49,0.0841968,0.0345552,-0.179771,-0.0675092
61,0.106927,0.00837173,0.126286,-0.000318217
41,-0.107874,-0.0573514,-0.2755,-0.246566
54,0.0192474,0.229892,-0.0972896,0.00394865
58,-0.0339982,0.121418,-0.20244,0.0103938
46,-0.12463,0.437342,-0.181203,-0.291535
90,0.0617435,0.167511,-0.164477,-0.294829
74,-0.00447634,0.0133631,-0.101687,-0.195731
20,-0.0332474,0.117189,-0.101946,0.0800356
37,-0.0696083,0.0196407,0.0299056,0.0179849
38,-0.0613334,0.176916,-0.11953,-0.0729487
44,0.0301321,-0.00162413,-0.113501,-0.153012
81,-0.186846,0.135323,-0.105413,-0.0280554
71,0.110091,0.137509,-0.0144935,-0.0706441
