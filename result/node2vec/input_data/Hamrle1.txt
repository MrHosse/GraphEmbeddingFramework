An algorithmic framework for representational learning on graphs. [Jul  3 2023]
================================================================================
Input graph path (-i:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph
Output graph path (-o:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.emb
Number of dimensions. Default is 128 (-d:)=4
Length of walk per source. Default is 80 (-l:)=80
Number of walks per source. Default is 10 (-r:)=10
Context size for optimization. Default is 10 (-k:)=10
Number of epochs in SGD. Default is 1 (-e:)=1
Return hyperparameter. Default is 1 (-p:)=1
Inout hyperparameter. Default is 1 (-q:)=1
Verbose output. (-v)=YES
Graph is directed. (-dr)=YES
Graph is weighted. (-w)=YES
Output random walks instead of embeddings. (-ow)=NO
Read 98 lines from /home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph

Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 

Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%

Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 39.06% 
Learning Progress: 78.12% 
22,-0.362398,-0.114064,-4.65628,-3.71963
2,-0.142598,-0.0229569,-4.57878,-3.71643
1,-0.153825,-0.0274882,-4.54289,-3.74055
4,-0.0203947,0.020015,-4.64216,-3.86978
8,-0.083084,0.0861587,-4.46601,-3.81024
14,0.0162399,0.343875,-4.21267,-3.64782
29,0.0245895,-0.0847139,-4.73945,-4.28554
17,-0.0142335,-0.181444,-4.50655,-4.15057
3,-0.13735,-0.0672678,-4.75401,-3.876
6,-0.448694,-0.179099,-4.64783,-3.78202
12,-0.282744,-0.21111,-4.69191,-3.83451
21,-0.0615133,0.0574525,-4.61345,-3.75149
5,-0.199023,0.00437316,-4.4689,-3.61317
7,-0.3757,-0.400468,-4.5728,-3.91166
11,-0.267286,-0.301116,-4.68605,-4.14049
20,-0.397138,-0.0868445,-4.83577,-4.02251
16,-0.0432926,0.188288,-4.37155,-3.76829
28,-0.347228,0.0715211,-4.25217,-3.70699
26,-0.172843,0.324524,-4.61274,-3.92298
30,-0.271904,0.229731,-4.88141,-4.28615
10,-0.400291,-0.231105,-4.76058,-3.99255
18,-0.327318,0.27211,-4.5363,-4.0842
24,-0.484088,-0.26431,-4.55958,-3.65034
19,-0.370478,-0.407698,-4.50966,-4.18224
23,-0.469971,-0.331858,-4.16618,-3.65494
9,-0.269493,-0.127174,-4.19316,-3.89415
32,-0.37392,0.291359,-4.56755,-3.76444
31,0.163449,0.0409993,-4.54969,-3.88762
25,0.128783,-0.0309371,-4.26613,-3.92054
27,-0.105812,-0.327143,-4.01206,-3.76024
15,-0.229419,-0.0399043,-4.27507,-3.82203
13,0.0905411,0.000308269,-4.09695,-3.76399
