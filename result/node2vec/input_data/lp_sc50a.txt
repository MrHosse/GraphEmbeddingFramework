An algorithmic framework for representational learning on graphs. [Jul  3 2023]
================================================================================
Input graph path (-i:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph
Output graph path (-o:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.emb
Number of dimensions. Default is 128 (-d:)=4
Length of walk per source. Default is 80 (-l:)=80
Number of walks per source. Default is 10 (-r:)=10
Context size for optimization. Default is 10 (-k:)=10
Number of epochs in SGD. Default is 1 (-e:)=1
Return hyperparameter. Default is 1 (-p:)=1
Inout hyperparameter. Default is 1 (-q:)=1
Verbose output. (-v)=YES
Graph is directed. (-dr)=YES
Graph is weighted. (-w)=YES
Output random walks instead of embeddings. (-ow)=NO
Read 160 lines from /home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph

Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 

Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%

Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 16.03% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 16.03% 
Learning Progress: 32.05% 
Learning Progress: 32.05% 
Learning Progress: 32.05% 
Learning Progress: 16.03% 
Learning Progress: 48.08% 
Learning Progress: 64.10% 
Learning Progress: 64.10% 
57,-0.157613,-0.00756135,0.447447,0.166964
0,0.996758,0.789699,3.96755,3.8059
56,0.0120979,-0.128348,0.596805,0.0520288
78,0.0854519,0.0731126,0.403574,0.0168807
60,0.153948,0.17595,0.333796,0.163965
61,0.126112,-0.00936237,0.370882,0.2037
46,-0.115542,-0.14933,0.278406,-0.576707
68,-0.0476874,0.155579,0.50996,-0.0930712
25,-0.0980755,-0.187621,0.459798,-0.421748
45,-0.112139,-0.0971239,0.419485,-0.281881
77,0.0103532,-0.125129,0.591689,0.0223971
2,-0.144762,-0.198809,0.0751156,-0.324591
31,-0.171731,-0.270644,0.183891,-0.545319
62,-0.0464692,-0.0544926,0.320436,0.14363
9,-0.265831,-0.226637,0.0740029,-0.456751
37,-0.0422715,-0.112834,0.171642,-0.349074
65,0.181032,0.0296948,0.376255,0.181091
70,0.208239,0.144365,0.194525,0.13004
13,-0.0586494,-0.0920109,-0.0429906,-0.31728
41,-0.140411,-0.103571,0.0185891,-0.144488
72,0.0966717,0.0654564,0.339793,0.224016
18,-0.0887199,-0.045698,0.263786,-0.477416
49,-0.0894802,-0.242882,0.388568,-0.307806
54,-0.0285817,-0.0189893,0.223617,0.184829
4,-0.0770311,-0.0740698,0.164289,-0.214275
35,-0.129855,-0.187605,0.315686,-0.429022
21,-0.279114,-0.210758,-0.021547,-0.467743
76,-0.0931949,-0.0211954,0.403373,0.0695611
40,-0.142045,-0.0481863,0.0537784,-0.281929
66,0.0544548,0.0195844,0.446608,0.124483
71,-0.0395429,0.0166303,0.242564,0.0855988
58,-0.00136037,0.0863792,0.150824,0.110082
5,-0.256744,-0.199764,0.336706,-0.412066
36,0.000725963,-0.14907,0.33644,-0.335548
67,0.0659216,-0.16255,0.497522,0.0438551
43,0.00534953,-0.172718,0.107109,-0.428874
75,0.0560768,0.0368378,0.506758,0.0892776
12,-0.0893441,-0.271292,0.0522389,-0.424574
42,-0.104042,-0.151134,-0.00521527,-0.41847
73,0.0948052,-0.150387,0.347554,0.20041
64,-0.0332514,-0.0313183,0.281005,0.0214282
29,-0.248011,-0.187087,0.204183,-0.186044
16,-0.112199,-0.191832,0.235591,-0.416214
6,-0.000993442,0.0263806,0.251557,-0.366735
32,-0.0313637,-0.182052,0.262901,-0.361503
26,-0.147417,-0.126348,0.30701,-0.151573
15,-0.117297,0.00461584,0.0976289,-0.521644
23,-0.158909,-0.0209832,0.0673358,-0.165381
51,0.00873782,-0.130493,0.513336,0.282079
20,-0.0123432,-0.199681,0.139061,-0.306283
48,-0.101269,-0.230651,0.305296,-0.247018
63,-0.0323634,0.0921316,0.462007,0.011969
8,-0.244045,-0.218013,-0.104569,-0.215224
27,-0.165032,-0.218029,0.344164,-0.427513
33,0.0524163,-0.166374,0.220063,-0.19045
19,-0.195469,-0.258807,0.187102,-0.37681
47,-0.124618,-0.206531,0.382174,-0.345837
69,0.223249,-0.0618715,0.271611,0.0255275
17,-0.112285,-0.304358,0.10905,-0.600318
34,0.0626427,-0.157782,0.076737,-0.288572
11,0.0221454,-0.190466,0.016418,-0.317856
44,-0.0335224,-0.0687364,0.401273,-0.318652
24,0.123826,-0.104371,0.131017,-0.570667
38,-0.106147,-0.139101,0.00696167,-0.372077
14,-0.21163,-0.171782,-0.0320738,-0.225785
53,0.0681784,0.0166784,0.371013,0.0388371
50,-0.0701947,0.00916105,0.316237,-0.30083
30,-0.124709,-0.0854295,0.375812,-0.411377
10,0.0332203,-0.11481,-0.0653164,-0.356027
39,-0.110212,-0.113954,0.286737,-0.127177
59,-0.210742,-0.105261,0.407662,0.0907676
28,-0.164296,-0.164833,0.273622,-0.209149
74,-0.015215,0.0641532,0.363801,0.204764
55,-0.0440272,0.0931304,0.383432,0.11209
1,-0.0372729,-0.105806,-0.00292824,-0.509224
52,-0.0507066,0.0333765,0.315736,0.249672
22,-0.0510521,-0.135039,0.195633,-0.0801281
7,-0.0931508,-0.140269,0.102815,-0.362771
3,-0.758723,-1.18801,-2.31841,-6.04112
