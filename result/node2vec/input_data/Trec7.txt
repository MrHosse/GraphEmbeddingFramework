An algorithmic framework for representational learning on graphs. [Jul  3 2023]
================================================================================
Input graph path (-i:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph
Output graph path (-o:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.emb
Number of dimensions. Default is 128 (-d:)=4
Length of walk per source. Default is 80 (-l:)=80
Number of walks per source. Default is 10 (-r:)=10
Context size for optimization. Default is 10 (-k:)=10
Number of epochs in SGD. Default is 1 (-e:)=1
Return hyperparameter. Default is 1 (-p:)=1
Inout hyperparameter. Default is 1 (-q:)=1
Verbose output. (-v)=YES
Graph is directed. (-dr)=YES
Graph is weighted. (-w)=YES
Output random walks instead of embeddings. (-ow)=NO
Read 147 lines from /home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph

Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 

Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%

Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 34.72% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 34.72% 
6,0.038113,-0.112459,-0.339244,-0.0879715
23,-0.0934817,0.118146,0.435413,0.213459
0,0.882969,-1.686,5.66326,1.70253
15,0.109094,0.0587205,0.312227,0.0939297
25,-0.101258,0.161295,0.408204,0.212435
19,-0.151817,0.184855,0.318472,0.112329
34,-0.103472,-0.118851,0.451021,0.036964
17,-0.130887,0.217932,0.3544,0.0768414
13,-0.228804,0.197743,0.475263,0.0828456
14,-0.167045,0.35412,0.207763,0.136314
27,-0.0769248,0.0504094,0.377993,0.0250788
10,0.0475332,-0.0485387,-0.218648,-0.0691855
9,0.00236856,-0.0978728,-0.309015,-0.0368811
11,-0.0482769,0.0140244,-0.0969272,0.00963283
31,-0.153456,0.0345927,0.66895,0.175817
4,0.166742,-0.016555,-0.426388,-0.254187
18,-0.0325993,0.256324,0.203015,0.0204433
12,0.0293161,0.0481556,0.42411,0.0926858
24,0.0595137,0.185959,0.370383,0.129745
16,-0.127198,0.108422,0.224495,0.164293
21,0.0202374,0.135251,0.227183,0.1543
33,-0.0497439,0.22205,0.443353,0.149632
7,-0.0121976,-0.0358395,-0.0391791,-0.0259895
5,0.142561,-0.0976159,-0.248248,-0.112131
2,0.143614,-0.0633036,-0.62752,-0.069249
3,0.0215124,0.0285248,-0.52419,-0.0892388
20,-0.0142052,0.184784,0.459738,0.100219
1,0.0173391,-0.0944632,-0.551214,-0.0699227
32,-0.181908,0.361727,0.176493,-0.0645117
30,-0.0281867,0.153963,0.45506,0.123291
22,-0.0811193,-0.0728817,0.480874,0.150792
8,0.00331754,-0.0782693,-0.328781,-0.0589277
28,-0.163968,0.171423,0.398031,0.0815987
29,-0.0546205,0.0419448,0.411127,0.00832687
36,0.00427706,0.119762,0.647179,0.195851
35,-0.0114357,0.108742,0.44667,0.026658
26,-0.175384,0.0203019,0.207051,0.0473017
