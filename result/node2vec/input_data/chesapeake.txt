An algorithmic framework for representational learning on graphs. [Jul  3 2023]
================================================================================
Input graph path (-i:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph
Output graph path (-o:)=/home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.emb
Number of dimensions. Default is 128 (-d:)=4
Length of walk per source. Default is 80 (-l:)=80
Number of walks per source. Default is 10 (-r:)=10
Context size for optimization. Default is 10 (-k:)=10
Number of epochs in SGD. Default is 1 (-e:)=1
Return hyperparameter. Default is 1 (-p:)=1
Inout hyperparameter. Default is 1 (-q:)=1
Verbose output. (-v)=YES
Graph is directed. (-dr)=YES
Graph is weighted. (-w)=YES
Output random walks instead of embeddings. (-ow)=NO
Read 170 lines from /home/hgholizadeh/GEF/embedding/node2vec_exe/temp_graph.graph

Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 
Preprocessing progress: 0.00% 

Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%
Walking Progress: 0.00%

Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 0.00% 
Learning Progress: 32.05% 
Learning Progress: 32.05% 
27,0.0248247,-0.00281692,0.0717577,-0.213763
14,0.125076,-0.0420146,0.0493114,0.0105968
3,-0.190833,-0.288265,0.406366,-0.154425
0,1.41901,2.22557,-8.01997,6.85519
26,-0.0489521,0.0682916,0.096607,-0.0406151
18,-0.0595384,-0.0333472,0.211841,-0.0143609
25,0.121256,0.0532269,0.205333,-0.0374815
28,-0.0659488,0.00475065,0.177811,-0.123061
9,0.114938,0.00866249,0.00138863,-0.0361785
2,-0.224524,-0.147261,0.210386,-0.018015
36,-0.237408,-0.352062,0.230182,-0.203072
23,0.0498814,-0.0744324,-0.0549465,-0.14083
19,-0.450978,-0.412857,0.0110458,-0.0647365
16,-0.102214,0.0473744,0.251973,-0.0401187
12,-0.0325557,-0.0913362,-0.0468719,-0.0633556
1,-0.13846,-0.330932,0.285344,-0.350388
38,-0.0640418,0.138022,0.155713,-0.0266659
11,0.140117,-0.0706419,-0.132157,0.054054
8,-0.21005,-0.53175,0.32983,-0.610292
13,0.205884,0.174251,0.129439,-0.0328568
21,-0.115376,-0.34895,0.0991249,-0.0919599
7,-0.175715,-0.163597,0.452156,-0.169548
6,0.0653284,0.0604009,0.157671,-0.00704927
5,0.16407,-0.0653562,-0.0959094,0.131769
15,0.0300002,-0.065973,0.174158,-0.10761
37,0.0596161,-0.124707,0.0131993,0.0261245
4,0.0612089,-0.134924,-0.159016,0.041553
30,0.0407106,-0.0687436,0.130049,-0.10654
22,-0.0716149,-0.245983,0.33725,-0.130734
20,-0.0732455,-0.248879,0.0408972,0.0343284
29,-0.0469195,-0.00775122,0.0563789,-0.0182337
35,0.0295463,0.0266046,0.00867992,-0.1278
32,-0.109001,-0.00884503,-0.13007,-0.147762
34,0.0278611,-0.0326513,-0.0277308,-0.0684592
33,0.00482522,-0.0737785,0.0214779,-0.00433504
24,-0.0721689,-0.236378,0.20026,-0.228267
31,-0.23903,-0.122556,0.069156,-0.202508
10,0.0436805,0.0674449,0.122618,-0.0955322
39,0.00888503,0.133637,0.124889,-0.176433
17,-0.101235,-0.0132698,0.0312062,0.0513156
